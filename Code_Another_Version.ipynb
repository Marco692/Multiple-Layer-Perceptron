{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_1_Marco.ipynb","provenance":[],"collapsed_sections":["1EeSQOcG66OV","bL6emm8CA2M5","Fs1lpxDp7455","Jo9pBHPgsOIr","tVr3Nr5_CQNs","SxIabCU4wOoT","e4QB-FZYwLct","XgZ5R9HLwFwz","W9sTnnY-wB7K","VtP96188v7xI"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **COMP5329 - Deep Learning Assignment 1**"],"metadata":{"id":"1EeSQOcG66OV"}},{"cell_type":"markdown","metadata":{"id":"7S9Dx7C_TQU6"},"source":["**Semester 1, 2022**\n","\n","**By:**\n","\n","* Xu Deng\n","* Yingbin Mo\n","* Yiran Zhang\n"]},{"cell_type":"code","source":["!cat /proc/cpuinfo"],"metadata":{"id":"cUBXvr-pltwk","executionInfo":{"status":"ok","timestamp":1649243776276,"user_tz":-480,"elapsed":336,"user":{"displayName":"Marco","userId":"15534896834138286062"}},"outputId":"3c05ce41-5608-4061-f5d0-b4788aa921c0","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["processor\t: 0\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 79\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 0\n","microcode\t: 0x1\n","cpu MHz\t\t: 2199.998\n","cache size\t: 56320 KB\n","physical id\t: 0\n","siblings\t: 2\n","core id\t\t: 0\n","cpu cores\t: 1\n","apicid\t\t: 0\n","initial apicid\t: 0\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 4399.99\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 1\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 79\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 0\n","microcode\t: 0x1\n","cpu MHz\t\t: 2199.998\n","cache size\t: 56320 KB\n","physical id\t: 0\n","siblings\t: 2\n","core id\t\t: 0\n","cpu cores\t: 1\n","apicid\t\t: 1\n","initial apicid\t: 1\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 4399.99\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n"]}]},{"cell_type":"markdown","source":["# **Setting the environment**"],"metadata":{"id":"bL6emm8CA2M5"}},{"cell_type":"code","source":["# load additional packages\n","import numpy as np\n","import copy\n","import math\n","from time import time\n","import matplotlib.pyplot as pl"],"metadata":{"id":"QRzTFzddA2aT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show running-config\n","!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jRmhfZXr7wgI","outputId":"dd36b5a9-fdbf-4277-e188-954cc35d2adc","executionInfo":{"status":"ok","timestamp":1649018683994,"user_tz":-480,"elapsed":490,"user":{"displayName":"Marco","userId":"15534896834138286062"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"]}]},{"cell_type":"markdown","source":["# **Loading the dataset**"],"metadata":{"id":"Fs1lpxDp7455"}},{"cell_type":"code","source":["# Code to download file into Colaboratory:\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"metadata":{"id":"fWzQaGlivqID"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download train and test data\n","id='10V_IhE8xdaHh_P1poCXYARb2b8TTBUri'\n","downloaded = drive.CreateFile({'id':id})\n","downloaded.GetContentFile('train_data.npy')\n","\n","id='12SvR08FPBzfrs4e3bpAP-OUW2jB9XMNb'\n","downloaded = drive.CreateFile({'id':id})\n","downloaded.GetContentFile('train_label.npy')\n","\n","id='1Umbpm9oDYYZaEye8WYlfcnKSZJEAlRxV'\n","downloaded = drive.CreateFile({'id':id})\n","downloaded.GetContentFile('test_data.npy')\n","\n","id='154-81EnE3cSNTJsW0PO1K6C6Z33NZvN2'\n","downloaded = drive.CreateFile({'id':id})\n","downloaded.GetContentFile('test_label.npy')"],"metadata":{"id":"uAwekmpAw1Pz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load train and test data\n","train_data = np.load(\"/content/train_data.npy\")\n","train_label = np.load(\"/content/train_label.npy\")\n","\n","test_data = np.load(\"/content/test_data.npy\")\n","test_label = np.load(\"/content/test_label.npy\")"],"metadata":{"id":"HU0g16eH4sHm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the shape of train and test data\n","print(\"The train data shape is: {}. The train label data shape is: {}\".format(train_data.shape, train_label.shape))\n","print(\"The test data shape is: {}. The test label data shape is: {}\".format(test_data.shape, test_label.shape))\n","\n","# Print the number and the form of labels \n","train_y = [x[0] for x in list(train_label)] \n","print(\"There are {} different labels in train data: {}\".format(len(set(train_y)), set(train_y)))\n","test_y = [x[0] for x in list(test_label)] \n","print(\"There are {} different labels in test data: {}\".format(len(set(test_y)), set(test_y)))"],"metadata":{"id":"aQa6vHHZm_fw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"78c564e5-7bc7-4b68-a60d-5f5683fa4aed","executionInfo":{"status":"ok","timestamp":1649018698095,"user_tz":-480,"elapsed":14,"user":{"displayName":"Marco","userId":"15534896834138286062"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The train data shape is: (50000, 128). The train label data shape is: (50000, 1)\n","The test data shape is: (10000, 128). The test label data shape is: (10000, 1)\n","There are 10 different labels in train data: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n","There are 10 different labels in test data: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"]}]},{"cell_type":"markdown","source":["# **Preprocessing data**"],"metadata":{"id":"Jo9pBHPgsOIr"}},{"cell_type":"code","source":["# # Normalize data to have zero mean and unit variance\n","# train_data = (train_data - train_data.mean(0)) / train_data.std(0)\n","# test_data = (test_data - test_data.mean(0)) / test_data.std(0)\n","\n","# # Convert label to one-hot encoding\n","# train_label = np.eye(10)[train_label.reshape(-1)] #eye会累加，所以只能运行一次\n","# test_label = np.eye(10)[test_label.reshape(-1)]"],"metadata":{"id":"XGlKTTz0sXWq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Activation**"],"metadata":{"id":"tVr3Nr5_CQNs"}},{"cell_type":"code","source":["class Activation(object):\n","    #sigmoid\n","    def __sigmoid(self, x):\n","        return 1.0 / (1.0 + np.exp(-x))\n","    def __sigmoid_deriv(self, a):\n","        return  a * (1 - a )\n","        \n","    #tanh\n","    def __tanh(self, x):\n","        return np.tanh(x)\n","    def __tanh_deriv(self, a):\n","        return 1.0 - a**2    \n","\n","    # relu\n","    def __relu(self, x):\n","        return np.maximum(0, x)\n","    def __relu_deriv(self, a):\n","        return np.where( a > 0, 1, 0)\n","\n","    #softmax\n","    def __softmax(self, x):\n","        e_x = np.exp(x - np.max(x, axis = -1,keepdims = True))\n","        return e_x / np.sum(e_x, axis = -1,keepdims = True)\n","\n","    def __init__(self, activation='relu'):\n","        if activation == 'sigmoid':\n","            self.f = self.__sigmoid\n","            self.f_deriv = self.__sigmoid_deriv\n","        elif activation == 'tanh':\n","            self.f = self.__tanh\n","            self.f_deriv = self.__tanh_deriv\n","        elif activation == 'relu':\n","            self.f = self.__relu\n","            self.f_deriv = self.__relu_deriv\n","        elif activation =='softmax':\n","            self.f = self.__softmax"],"metadata":{"id":"NGrI6D8xt8Yk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Layer**"],"metadata":{"id":"SxIabCU4wOoT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcY8qr0lsCpB"},"outputs":[],"source":["class Layer(object):\n","    \n","    def __init__(self, n_in, n_out, optimizer, activation = 'relu'):\n","        self.input = None\n","        self.linear_output = None\n","        self.output = None\n","        #定义激活函数及其导数\n","        self.activation = Activation(activation).f\n","        if activation == 'softmax': #因为结合cross entropy可以更快算出delta\n","            self.activation_deriv = None\n","        else:\n","            self.activation_deriv = Activation(activation).f_deriv\n","\n","        # 初始化该层网络的权重和偏置       \n","        if activation == 'relu' or activation == 'leakyrelu': #kaiming for relu or leakyrelu\n","            self.W = np.random.uniform(\n","                low=-np.sqrt(6. / (n_in)), \n","                high=np.sqrt(6. / (n_in)),\n","                size=(n_in, n_out))\n","        else: #Xavier for sigmoid or tanh\n","            self.W = np.random.uniform(\n","                low=-np.sqrt(6. / (n_in + n_out)), \n","                high=np.sqrt(6. / (n_in + n_out)),\n","                size=(n_in, n_out))\n","        if activation == 'sigmoid':\n","            self.W *=4\n","        self.b = np.zeros(n_out,)\n","\n","        # 初始化优化器 TODO\n","        self.opt_w = copy.copy(optimizer)\n","        self.opt_b = copy.copy(optimizer)\n","        \n","    def forward(self, input, train = True):\n","        self.input = input\n","        #计算输出\n","        self.linear_output = np.dot(input, self.W)+self.b\n","        self.output = self.activation(self.linear_output)\n","        return self.output\n","        \n","    def backward(self, delta):\n","        if not self.activation_deriv is None: #最后一层的delta无需求导即可得\n","            delta = delta *self.activation_deriv(self.linear_output)\n","        W_original = self.W\n","        # 计算梯度\n","        grad_W = np.dot(self.input.T, delta)\n","        grad_d = np.sum(delta, axis = 0 ,keepdims = True)\n","        # 更新权重和偏置#TODO\n","        self.W = self.opt_w.update(self.W, grad_W)\n","        self.b = self.opt_b.update(self.b, grad_d)\n","        # 更新梯度\n","        delta = np.dot(delta, W_original.T)    \n","        return delta\n","                "]},{"cell_type":"markdown","source":["# **DropoutLayer**"],"metadata":{"id":"e4QB-FZYwLct"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUEhCV6xsCpB"},"outputs":[],"source":["class DropoutLayer(object):\n","    def __init__(self, ratio = 0.5):\n","        self.ratio = ratio\n","        self.mask = None\n","        \n","    def forward(self, X ,train = True):\n","        if train:\n","            self.mask = np.random.uniform(size = X.shape)>self.ratio\n","            return X * self.mask\n","        else:\n","            return X\n","            \n","    def backward(self, delta):\n","        return delta * self.mask"]},{"cell_type":"markdown","source":["# **BNLayer**"],"metadata":{"id":"XgZ5R9HLwFwz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7ZZK8yasCpC"},"outputs":[],"source":["class BNLayer(object):\n","    def __init__(self, gamma, beta, optimizer, momentum = 0.9):\n","        self.Xm = None\n","        self.Xv = None\n","        self.Xn =None\n","        self.bn_ga = gamma\n","        self.bn_be = beta\n","        \n","        self.opt = optimizer\n","        self.Mo = momentum\n","        self.opt_ga = copy.copy(optimizer)\n","        self.opt_be = copy.copy(optimizer)\n","        \n","    def forward(self, X ,train = True):\n","         # 初始化\n","        if self.Xm is None:\n","            self.Xm = np.mean (X, axis = 0)\n","            self.Xv = np.var (X, axis =0)\n","        if train: \n","            m = np.mean (X, axis = 0)\n","            self.Xm = self.Mo * self.Xm + (1 - self.Mo) * m #指数移动平均\n","            v = np.var (X, axis = 0)\n","            self.Xv = self.Mo * self.Xv + (1 - self.Mo) * v\n","        else:\n","            m = self.Xm\n","            v = self.Xv\n","        #求出经过normalise后的值\n","        self.di = X - m\n","        eps_bn = np.finfo(float).eps\n","        self.Xs = np.sqrt(np.maximum(v, eps_bn))\n","        self.Xn = self.di / self.Xs\n","        output = self.bn_ga * self.Xn + self.bn_be\n","        return output\n","    \n","    def backward (self, delta):\n","        bn_ga_original = self.bn_ga\n","        bn_ga_grad = np.sum(delta * self.Xn, axis = 0)\n","        bn_be_grad = np.sum(delta, axis = 0)\n","        N, _ = delta.shape\n","        #更新ga和be\n","        self.bn_ga = self.opt_ga.update(self.bn_ga, bn_ga_grad)\n","        self.bn_be = self.opt_be.update(self.bn_be,bn_be_grad)\n","        eps_bn = np.finfo(float).eps\n","        dXn = delta * bn_ga_original\n","        dv1=pow((self.Xv + eps_bn),-3/2)\n","        dv2=np.sum(dXn * self.di, axis = 0) \n","        dv = -(1/2)*dv1*dv2\n","        dm1=np.sum(dXn * (1/self.Xs), axis = 0) \n","        dm2=dv * (1/N) * np.sum(-2 * self.di, axis = 0) \n","        dm =  dm1+dm2\n","        #更新delta\n","        d1=dm/N\n","        d2=dv * 2 / N* self.di \n","        d3=dXn * pow(self.Xs, -1)\n","        delta =d1+d2+d3\n","        return delta"]},{"cell_type":"markdown","source":["# **Optimizer**"],"metadata":{"id":"W9sTnnY-wB7K"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KwYs8pfysCpC"},"outputs":[],"source":["class Optimizer (object):\n","    def __init__(self, lr = 0.001, momentum = 0.9, rho=0.9, weight_decay = 1e-2, mode='M_S'):\n","        self.lr = lr\n","        self.Mo = momentum\n","        self.grad = None\n","        self.Eg = None # Running average of the square gradients at w\n","        self.rho = rho\n","        self.weight_decay = weight_decay\n","        self.mode = mode\n","        \n","    def update(self, w, delta):\n","        if self.mode is 'M_S':  #if momentum =0  ->SGD\n","            if self.grad is None:\n","                self.grad = np.zeros(w.shape)\n","            self.grad = self.Mo * self.grad + (1 - self.Mo) * delta\n","            w = w * (1- self.weight_decay) - self.lr * self.grad\n","            return w\n","\n","        if self.mode is 'RMS':\n","            if self.Eg is None:\n","                self.Eg = np.zeros(w.shape)\n","            self.Eg = self.rho * self.Eg + (1 - self.rho) * np.power(delta, 2)\n","            w = w*(1- self.weight_decay) - self.lr *  delta / np.sqrt(self.Eg + np.finfo(float).eps)\n","            return  w"]},{"cell_type":"markdown","source":["# **MLP**"],"metadata":{"id":"VtP96188v7xI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"t90Dx79NsCpC"},"outputs":[],"source":["class MLP (object) :\n","    def __init__(self, n_in, n_out, layer, \n","                optimizer,\n","                activation, #['relu','relu','softmax\"]\n","                BN = False,\n","                Dropout = False,\n","                dropout_ratio = None):\n","        self.layers = []\n","        self.activation = activation\n","        self.opt = optimizer\n","        self.lr = self.opt.lr\n","        self.n_out = n_out\n","\n","        #添加第一层隐藏层\n","        self.layers.append(Layer(n_in, layer[0], optimizer, activation[0]))\n","        if Dropout:\n","            self.layers.append(DropoutLayer(dropout_ratio[0]))\n","        if BN:\n","            self.layers.append(BNLayer(np.ones((1, layer[0])), np.zeros((1,layer[0])),optimizer))\n","        #添加剩余的隐藏层\n","        for i in range(1,len(layer)):\n","            self.layers.append(Layer(layer[i-1], layer[i], optimizer, activation[i]))\n","            if Dropout:\n","                self.layers.append(DropoutLayer(dropout_ratio[i]))\n","            if BN:\n","                self.layers.append(BNLayer(np.ones((1, layer[i])), np.zeros((1,layer[i])),optimizer))\n","        #添加输出层 \n","        self.layers.append(Layer(layer[-1],n_out,optimizer,activation[-1]))\n","        \n","    def criterion_CEL (self, y, y_pred):\n","        y_pred = np.maximum(y_pred, np.finfo(float).eps)\n","        y_oh = np.eye(self.n_out)[y].reshape(-1,self.n_out)\n","        loss = -np.sum(np.multiply(y_oh, np.log(y_pred)))\n","        # 此处结合了softmax\n","        delta = y_pred - y_oh\n","        return loss, delta\n","    \n","    def forward(self, input, train = True):\n","        for layer in self.layers:\n","            output = layer.forward(input, train)\n","            input = output\n","        return output\n","    \n","    def backward (self, delta):\n","        for layer in reversed(self.layers):\n","            delta = layer.backward (delta)\n","\n","    def val_predict (self, X, y) :\n","        y_pred_val = self.forward (X, train = False)\n","        y_pred_val=np.argmax(y_pred_val, axis = 1).reshape(-1,1) \n","        loss_val, _ = self.criterion_CEL (y, y_pred_val)\n","        val_mean_loss = loss_val/X.shape[0]\n","        true_num = np.sum( y_pred_val== y, axis = 0)\n","        val_acc = float(true_num / X.shape[0])\n","        return val_mean_loss, val_acc\n","    \n","    def test_predict (self, X, y) :\n","        y_pred_test = self.forward (X, train = False)\n","        y_pred_test = np.argmax(y_pred_test, axis = 1).reshape(-1,1) \n","        loss_test, _ = self.criterion_CEL (y, y_pred_test)\n","        test_mean_loss = loss_test/X.shape[0]\n","        true_num = np.sum( y_pred_test== y, axis = 0)\n","        test_acc = float(true_num / X.shape[0])\n","        print(f'Test_loss: {test_mean_loss:.4f}\\tTest_acc: {(test_acc*100):.2f}%')\n","        \n","\n","    def fit(self, X, y, epochs = 100, batch_size = 100):\n","        train_loss_list = []\n","        train_acc_list = []\n","        val_loss_list = []\n","        val_acc_list = []\n","        for epoch in range (epochs) :\n","            #改变学习率\n","            # if epoch == int(epochs*1/3):\n","            #     self.lr = self.lr/5\n","            #     self.opt = Optimizer(lr = self.lr)\n","            # elif epoch == int(epochs*2/3):\n","            #     self.lr = self.lr/5\n","            #     self.opt = Optimizer(lr = self.lr)\n","            shuffle = np.arange(X.shape[0])\n","            np.random.shuffle(shuffle)\n","            X_ran = X[shuffle]\n","            y_ran = y[shuffle]\n","            ins = int(0.8*len(X_ran))\n","            X_train = X_ran[ :ins]\n","            y_train  =y_ran[ :ins]\n","            X_val = X_ran[ins: ]\n","            y_val = y_ran [ins: ]\n","                \n","            train_loss_one_epoch= 0\n","            y_pred_one_epoch = []\n","\n","            iteration = X_train.shape[0] // batch_size\n","            begin = time()\n","            for it in range(iteration):\n","                start = it * batch_size\n","                stop = min((it+1) * batch_size, len(X_train))\n","                X_batch = X_ran[start : stop]\n","                y_batch = y_ran[start : stop]\n","\n","                # 向前传播\n","                y_pred = self.forward(X_batch)#y_pred.shape 1000x10\n","\n","                # 计算损失梯度\n","                loss, delta = self.criterion_CEL(y_batch, y_pred)\n","\n","                #反向传播\n","                self.backward(delta) \n","                train_loss_one_epoch += loss#float 累加      \n","                y_pred_one_epoch.extend(y_pred) #list 堆叠\n","\n","            train_mean_loss = train_loss_one_epoch / len(X_ran)\n","            train_loss_list.append(train_mean_loss) #含有epoch个loss的list\n","\n","            y_pred_one_epoch = np.array(y_pred_one_epoch) #y_pred_one_epoch 40000x10\n","            y_pred = y_pred_one_epoch.argmax(1).reshape (-1, 1) #y_pred 40000x1\n","            z=0\n","            for k in range(len(y_pred)):\n","                if y_pred[k] == y_train[k]:\n","                    z +=1\n","            train_acc = z/ len(X_ran)\n","            train_acc_list.append(train_acc)\n","\n","            val_mean_loss, val_acc = self.val_predict(X_val, y_val)\n","            val_loss_list.append(val_mean_loss)\n","            val_acc_list.append(val_acc)\n","\n","                                                        \n","            print (f\"Epoch: {(epoch+1):02d} ->\\tTrain_loss: {train_loss_list[-1]:.4f}\\tTrain_acc: {(train_acc_list[-1]*100):.2f}%\\t丨\\tVal_loss: {val_loss_list[-1]:.4f}\\tVal_acc: {(val_acc_list[-1]*100):.2f}%\" )\n","        \n","        return train_loss_list, train_acc_list, val_loss_list, val_acc_list"]},{"cell_type":"markdown","source":["# **Training**"],"metadata":{"id":"6R6Fb7bfv2zi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eolbPdaJsCpD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649019198099,"user_tz":-480,"elapsed":107016,"user":{"displayName":"Marco","userId":"15534896834138286062"}},"outputId":"9e34e005-3b31-4550-8dc0-7a4b7c2aa03b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 ->\tTrain_loss: 1.6535\tTrain_acc: 24.01%\t丨\tVal_loss: 2.7039\tVal_acc: 41.12%\n","Epoch: 02 ->\tTrain_loss: 1.4064\tTrain_acc: 30.57%\t丨\tVal_loss: 2.9759\tVal_acc: 44.36%\n","Epoch: 03 ->\tTrain_loss: 1.3165\tTrain_acc: 33.40%\t丨\tVal_loss: 2.4578\tVal_acc: 47.83%\n","Epoch: 04 ->\tTrain_loss: 1.2598\tTrain_acc: 35.60%\t丨\tVal_loss: 2.4899\tVal_acc: 48.72%\n","Epoch: 05 ->\tTrain_loss: 1.2171\tTrain_acc: 36.72%\t丨\tVal_loss: 3.4820\tVal_acc: 51.02%\n","Epoch: 06 ->\tTrain_loss: 1.1809\tTrain_acc: 37.96%\t丨\tVal_loss: 2.8715\tVal_acc: 51.29%\n","Epoch: 07 ->\tTrain_loss: 1.1505\tTrain_acc: 39.11%\t丨\tVal_loss: 2.6134\tVal_acc: 52.59%\n","Epoch: 08 ->\tTrain_loss: 1.1277\tTrain_acc: 39.95%\t丨\tVal_loss: 2.1749\tVal_acc: 54.35%\n","Epoch: 09 ->\tTrain_loss: 1.1034\tTrain_acc: 40.75%\t丨\tVal_loss: 2.6101\tVal_acc: 54.46%\n","Epoch: 10 ->\tTrain_loss: 1.0820\tTrain_acc: 41.38%\t丨\tVal_loss: 2.3592\tVal_acc: 55.40%\n","Epoch: 11 ->\tTrain_loss: 1.0659\tTrain_acc: 42.10%\t丨\tVal_loss: 3.0318\tVal_acc: 56.40%\n","Epoch: 12 ->\tTrain_loss: 1.0436\tTrain_acc: 42.86%\t丨\tVal_loss: 2.6339\tVal_acc: 57.05%\n","Epoch: 13 ->\tTrain_loss: 1.0285\tTrain_acc: 43.52%\t丨\tVal_loss: 2.1082\tVal_acc: 57.37%\n","Epoch: 14 ->\tTrain_loss: 1.0152\tTrain_acc: 43.72%\t丨\tVal_loss: 2.9121\tVal_acc: 58.09%\n","Epoch: 15 ->\tTrain_loss: 0.9978\tTrain_acc: 44.47%\t丨\tVal_loss: 3.2205\tVal_acc: 58.94%\n","Epoch: 16 ->\tTrain_loss: 0.9868\tTrain_acc: 45.11%\t丨\tVal_loss: 3.3574\tVal_acc: 60.05%\n","Epoch: 17 ->\tTrain_loss: 0.9716\tTrain_acc: 45.49%\t丨\tVal_loss: 2.3700\tVal_acc: 60.38%\n","Epoch: 18 ->\tTrain_loss: 0.9563\tTrain_acc: 46.09%\t丨\tVal_loss: 2.3631\tVal_acc: 60.71%\n","Epoch: 19 ->\tTrain_loss: 0.9474\tTrain_acc: 46.33%\t丨\tVal_loss: 2.7554\tVal_acc: 61.85%\n","Epoch: 20 ->\tTrain_loss: 0.9319\tTrain_acc: 46.93%\t丨\tVal_loss: 3.0691\tVal_acc: 61.59%\n"]}],"source":["optimizer = Optimizer(lr = 0.001, momentum = 0.8, rho=0.9, weight_decay = 1e-3, mode='RMS')\n","n_in = train_data.shape[1]\n","n_out = len(np.unique(train_label))\n","layer = [256,512]\n","activation = ['relu','relu','softmax']\n","model = MLP(n_in, n_out, layer, # [128, 256, 512]\n","                optimizer,\n","                activation, #['relu','relu','softmax\"]\n","                BN = True,\n","                Dropout = True,\n","                dropout_ratio =[0.2,0.2])\n","\n","train_loss_list, train_acc_list, val_loss_list, val_acc_list = model.fit (train_data, train_label, epochs =20, batch_size = 1000)"]},{"cell_type":"code","source":["model.test_predict(test_data, test_label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RgTudfFJFpa","executionInfo":{"status":"ok","timestamp":1649019210623,"user_tz":-480,"elapsed":804,"user":{"displayName":"Marco","userId":"15534896834138286062"}},"outputId":"e0f0dbfb-02fe-4c72-d7bd-b379d2f62c93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test_loss: 3.1517\tTest_acc: 55.21%\n"]}]}]}